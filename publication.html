<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Gang Niu (Research Scientist at RIKEN-AIP)</title>
    <base href="https://niug1984.github.io/publication.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Gang Niu (Research Scientist at RIKEN-AIP)</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
    <!-- <div class="menu-item"><a href="software.html">Software</a></div> -->
    <div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">

    <h1 style="margin-top: 0em">Publications</h1><br>
    <p>[ <a href="#conference">Conference Papers</a>,
        <a href="#journal">Journal Articles</a>,
        <a href="#workshop">Workshop Presentations</a>,
        <a href="#thesis">Theses</a> ]</p>
    <p>An asterisk (*) beside authors' names indicates equal contributions.</p>

    <div>
        <h2><hr>Preprints</h2>
        <ul>
            <li><p>
                A. Berthon, B. Han, G. Niu, T. Liu, and M. Sugiyama.<br>
                Confidence scores make instance-dependent label-noise learning possible.<br>
                [ <a href="https://openreview.net/forum?id=SyevDaVYwr" target="_blank">OpenReview</a> ]
            </p></li>
            <li><p>
                H. Yang, Q. Yao, B. Han, and G. Niu.<br>
                Searching to exploit memorization effect in learning from corrupted labels.<br>
                [ <a href="http://arxiv.org/abs/1911.02377" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                J. Zhang, B. Han, G. Niu, T. Liu, and M. Sugiyama.<br>
                Where is the bottleneck of adversarial learning with unlabeled data?<br>
                [ <a href="http://arxiv.org/abs/1911.08696" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                A. Jacovi, G. Niu, Y. Goldberg, and M. Sugiyama.<br>
                Scalable evaluation and improvement of document set expansion via neural positive-unlabeled learning.<br>
                [ <a href="http://arxiv.org/abs/1910.13339" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                N. Lu, T. Zhang, G. Niu, and M. Sugiyama.<br>
                Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach.<br>
                [ <a href="http://arxiv.org/abs/1910.08974" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                W. Xu, G. Niu, A. Hyvärinen, and M. Sugiyama.<br>
                Direction matters: On influence-preserving graph summarization and max-cut principle for directed graphs.<br>
                [ <a href="http://arxiv.org/abs/1907.09588" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                Y. Pan, W. Chen, G. Niu, I. W. Tsang, and M. Sugiyama.<br>
                Fast and robust rank aggregation against model misspecification.<br>
                [ <a href="http://arxiv.org/abs/1905.12341" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                F. Liu, J. Lu, B. Han, G. Niu, G. Zhang, and M. Sugiyama.<br>
                Butterfly: A panacea for all difficulties in wildly unsupervised domain adaptation.<br>
                [ <a href="http://arxiv.org/abs/1905.07720" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                C.-Y. Hsieh, M. Xu, G. Niu, H.-T. Lin, and M. Sugiyama.<br>
                A pseudo-label method for coarse-to-fine multi-label learning with limited supervision.<br>
                [ <a href="https://openreview.net/forum?id=rylVYjqHdN" target="_blank">OpenReview</a> ]
            </p></li>
            <li><p>
                M. Xu, B. Li, G. Niu, B. Han, and M. Sugiyama.<br>
                Revisiting sample selection approach to positive-unlabeled learning: Turning unlabeled data into positive rather than negative.<br>
                [ <a href="http://arxiv.org/abs/1901.10155" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                B. Han, G. Niu, J. Yao, X. Yu, M. Xu, Q. Yao, I. W. Tsang, and M. Sugiyama.<br>
                Pumpout: A meta approach to robust deep learning with noisy labels.<br>
                [ <a href="http://arxiv.org/abs/1809.11008" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                M. Kato, L. Xu, G. Niu, and M. Sugiyama.<br>
                Alternate estimation of a classifier and the class-prior from positive and unlabeled data.<br>
                [ <a href="http://arxiv.org/abs/1809.05710" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                M. Xu, G. Niu, B. Han, I. W. Tsang, Z.-H. Zhou, and M. Sugiyama.<br>
                Matrix co-completion for multi-label classification with missing features and labels.<br>
                [ <a href="http://arxiv.org/abs/1805.09156" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                T. Sakai, G. Niu, and M. Sugiyama.<br>
                Information-theoretic representation learning for positive-unlabeled classification.<br>
                [ <a href="http://arxiv.org/abs/1710.05359" target="_blank">arXiv</a> ]
            </p></li>
        </ul>
    </div>

    <div>
        <h2><hr><a name="conference"></a>Conference Papers (full review)</h2>
        <ol>
            <li><p>
                C. Li, M. E. Khan, Z. Sun, G. Niu, B. Han, S. Xie, and Q. Zhao.<br>
                Beyond unfolding: Exact recovery of latent convex tensor decomposition under reshuffling.<br>
                In <i>Proceedings of <a href="http://aaai.org/Conferences/AAAI-20/" target="_blank">34th AAAI Conference on Artificial Intelligence (AAAI'20)</a></i>,
                to appear.<br>
                [ <a href="paper/li_aaai20.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                L. Xu, J. Honda, G. Niu, and M. Sugiyama.<br>
                Uncoupled regression from pairwise comparison data.<br>
                In <i><a href="http://nips.cc/Conferences/2019/" target="_blank">Advances in Neural Information Processing Systems 32 (NeurIPS'19)</a></i>,
                to appear.<br>
                [ <a href="paper/xu_neurips19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, and M. Sugiyama.<br>
                Are anchor points really indispensable in label-noise learning?<br>
                In <i><a href="http://nips.cc/Conferences/2019/" target="_blank">Advances in Neural Information Processing Systems 32 (NeurIPS'19)</a></i>,
                to appear.<br>
                [ <a href="paper/xia_neurips19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                Y.-G. Hsieh, G. Niu, and M. Sugiyama.<br>
                Classification from positive, unlabeled and biased negative data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2019" target="_blank">36th International Conference on Machine Learning (ICML'19)</a></i>,
                PMLR, vol. 97, pp. 2820--2829, Long Beach, California, USA, Jun 9--15, 2019.<br>
                [ <a href="paper/hsieh_icml19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Ishida, G. Niu, A. K. Menon, and M. Sugiyama.<br>
                Complementary-label learning for arbitrary losses and models.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2019" target="_blank">36th International Conference on Machine Learning (ICML'19)</a></i>,
                PMLR, vol. 97, pp. 2971--2980, Long Beach, California, USA, Jun 9--15, 2019.<br>
                [ <a href="paper/ishida_icml19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                X. Yu, B. Han, J. Yao, G. Niu, I. W. Tsang, and M. Sugiyama.<br>
                How does disagreement help generalization against label corruption?<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2019" target="_blank">36th International Conference on Machine Learning (ICML'19)</a></i>,
                PMLR, vol. 97, pp. 7164--7173, Long Beach, California, USA, Jun 9--15, 2019.<br>
                [ <a href="paper/yu_icml19.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                N. Lu, G. Niu, A. K. Menon, and M. Sugiyama.<br>
                On the minimal supervision for training any binary classifier from only unlabeled data.<br>
                In <i>Proceedings of <a href="https://iclr.cc/Conferences/2019/" target="_blank">7th International Conference on Learning Representations (ICLR'19)</a></i>,
                18 pages, New Orleans, Louisiana, USA, May 6--9, 2019.<br>
                [ <a href="paper/lu_iclr19.pdf" target="_blank">paper</a>,
                <a href="https://openreview.net/forum?id=B1xWcj0qYm" target="_blank">OpenReview</a> ]
            </p></li>
            <li><p>
                T. Ishida, G. Niu, and M. Sugiyama.<br>
                Binary classification for positive-confidence data.<br>
                In <i><a href="http://nips.cc/Conferences/2018/" target="_blank">Advances in Neural Information Processing Systems 31 (NeurIPS'18)</a></i>,
                pp. 5917--5928, Montreal, Quebec, Canada, Dec 2--8, 2018.<br>
                <font color="#008800">(This paper was selected for spotlight presentation;
                there are 168 spotlights among 1011 acceptance out of 4856 submissions)</font><br>
                [ <a href="paper/ishida_neurips18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Han*, J. Yao*, G. Niu, M. Zhou, I. W. Tsang, Y. Zhang, and M. Sugiyama.<br>
                Masking: A new perspective of noisy supervision.<br>
                In <i><a href="http://nips.cc/Conferences/2018/" target="_blank">Advances in Neural Information Processing Systems 31 (NeurIPS'18)</a></i>,
                pp. 5836--5846, Montreal, Quebec, Canada, Dec 2--8, 2018.<br>
                [ <a href="paper/han_neurips18a.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Han*, Q. Yao*, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama.<br>
                Co-teaching: Robust training of deep neural networks with extremely noisy labels.<br>
                In <i><a href="http://nips.cc/Conferences/2018/" target="_blank">Advances in Neural Information Processing Systems 31 (NeurIPS'18)</a></i>,
                pp. 8527--8537, Montreal, Quebec, Canada, Dec 2--8, 2018.<br>
                [ <a href="paper/han_neurips18b.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                W. Hu, G. Niu, I. Sato, and M. Sugiyama.<br>
                Does distributionally robust supervised learning give robust classifiers?<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2018" target="_blank">35th International Conference on Machine Learning (ICML'18)</a></i>,
                PMLR, vol. 80, pp. 2029--2037, Stockholm, Sweden, Jul 10--15, 2018.<br>
                [ <a href="paper/hu_icml18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                H. Bao, G. Niu, and M. Sugiyama.<br>
                Classification from pairwise similarity and unlabeled data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2018" target="_blank">35th International Conference on Machine Learning (ICML'18)</a></i>,
                PMLR, vol. 80, pp. 452--461, Stockholm, Sweden, Jul 10--15, 2018.<br>
                [ <a href="paper/bao_icml18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                S.-J. Huang, M. Xu, M.-K. Xie, M. Sugiyama, G. Niu, and S. Chen.<br>
                Active feature acquisition with supervised matrix completion.<br>
                In <i>Proceedings of <a href="http://www.kdd.org/kdd2018/" target="_blank">24th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'18)</a></i>,
                pp. 1571--1579, London, UK, Aug 19--23, 2018.<br>
                [ <a href="paper/huang_kdd18.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama.<br>
                Positive-unlabeled learning with non-negative risk estimator.<br>
                In <i><a href="http://nips.cc/Conferences/2017/" target="_blank">Advances in Neural Information Processing Systems 30 (NeurIPS'17)</a></i>,
                pp. 1674--1684, Long Beach, California, USA, Dec 4--9, 2017.<br>
                <font color="#008800">(This paper was selected for oral presentation;
                there are 40 orals among 678 acceptance out of 3240 submissions)</font><br>
                [ <a href="paper/kiryo_nips17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Ishida, G. Niu, W. Hu, and M. Sugiyama.<br>
                Learning from complementary labels.<br>
                In <i><a href="http://nips.cc/Conferences/2017/" target="_blank">Advances in Neural Information Processing Systems 30 (NeurIPS'17)</a></i>,
                pp. 5644--5654, Long Beach, California, USA, Dec 4--9, 2017.<br>
                [ <a href="paper/ishida_nips17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                H. Shiino, H. Sasaki, G. Niu, and M. Sugiyama.<br>
                Whitening-free least-squares non-Gaussian component analysis.<br>
                In <i>Proceedings of <a href="http://acml-conf.org/2017/" target="_blank">9th Asian Conference on Machine Learning (ACML'17)</a></i>,
                PMLR, vol. 77, pp. 375--390, Seoul, Korea, Nov 15--17, 2017.<br>
                <font color="#008800">(This paper was selected for Best Paper Runner-up Award)</font><br>
                [ <a href="paper/shiino_acml17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Sakai, M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Semi-supervised classification based on classification from positive and unlabeled data.<br>
                In <i>Proceedings of <a href="http://icml.cc/Conferences/2017" target="_blank">34th International Conference on Machine Learning (ICML'17)</a></i>,
                PMLR, vol. 70, pp. 2998--3006, Sydney, Australia, Aug 6--11, 2017.<br>
                [ <a href="paper/sakai_icml17.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, M. C. du Plessis, T. Sakai, Y. Ma, and M. Sugiyama.<br>
                Theoretical comparisons of positive-unlabeled learning against positive-negative learning.<br>
                In <i><a href="http://nips.cc/Conferences/2016/" target="_blank">Advances in Neural Information Processing Systems 29 (NeurIPS'16)</a></i>,
                pp. 1199--1207, Barcelona, Spain, Dec 5--10, 2016.<br>
                [ <a href="paper/niu_nips16.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                H. Sasaki, G. Niu, and M. Sugiyama.<br>
                Non-Gaussian component analysis with log-density gradient estimation.<br>
                In <i>Proceedings of <a href="http://www.aistats.org/aistats2016/" target="_blank">18th International Conference on Artificial Intelligence and Statistics (AISTATS'16)</a></i>,
                PMLR, vol. 51, pp. 1177--1185, Cadiz, Spain, May 9--11, 2016.<br>
                [ <a href="paper/sasaki_aistats16.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Zhao, G. Niu, N. Xie, J. Yang, and M. Sugiyama.<br>
                Regularized policy gradients: Direct variance reduction in policy gradient estimation.<br>
                In <i>Proceedings of <a href="http://acml-conf.org/2015/" target="_blank">7th Asian Conference on Machine Learning (ACML'15)</a></i>,
                PMLR, vol. 45, pp. 333--348, Hong Kong, China, Nov 20--22, 2015.<br>
                [ <a href="paper/zhao_acml15.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Class-prior estimation for learning from positive and unlabeled data.<br>
                In <i>Proceedings of <a href="http://acml-conf.org/2015/" target="_blank">7th Asian Conference on Machine Learning (ACML'15)</a></i>,
                PMLR, vol. 45, pp. 221--236, Hong Kong, China, Nov 20--22, 2015.<br>
                [ <a href="paper/duplessis_acml15.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Convex formulation for learning from positive and unlabeled data.<br>
                In <i>Proceedings of <a href="http://icml.cc/2015/" target="_blank">32nd International Conference on Machine Learning (ICML'15)</a></i>,
                PMLR, vol. 37, pp. 1386--1394, Lille, France, Jul 6--11, 2015.<br>
                [ <a href="paper/duplessis_icml15.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Analysis of learning from positive and unlabeled data.<br>
                In <i><a href="http://nips.cc/Conferences/2014/" target="_blank">Advances in Neural Information Processing Systems 27 (NeurIPS'14)</a></i>,
                pp. 703--711, Montreal, Quebec, Canada, Dec 8--13, 2014.<br>
                [ <a href="paper/duplessis_nips14.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. C. du Plessis, and M. Sugiyama.<br>
                Transductive learning with multi-class volume approximation.<br>
                In <i>Proceedings of <a href="http://icml.cc/2014/" target="_blank">31st International Conference on Machine Learning (ICML'14)</a></i>,
                PMLR, vol. 32, no. 2, pp. 1377--1385, Beijing, China, Jun 21--26, 2014.<br>
                [ <a href="paper/niu_icml14.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis, G. Niu, and M. Sugiyama.<br>
                Clustering unclustered data: Unsupervised binary labeling of two datasets having different class balances.<br>
                In <i>Proceedings of 2013 Conference on Technologies and Applications of Artificial Intelligence (TAAI'13)</i>,
                pp. 1--6, Taipei, Taiwan, Dec 6--8, 2013.<br>
                <font color="#008800">(This paper was selected for Best Paper Award)</font><br>
                [ <a href="paper/duplessis_taai13.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, W. Jitkrittum, B. Dai, H. Hachiya, and M. Sugiyama.<br>
                Squared-loss mutual information regularization: A novel information-theoretic approach to semi-supervised learning.<br>
                In <i>Proceedings of <a href="http://icml.cc/2013/" target="_blank">30th International Conference on Machine Learning (ICML'13)</a></i>,
                PMLR, vol. 28, no. 3, pp. 10--18, Atlanta, Georgia, USA, Jun 16--21, 2013.<br>
                [ <a href="paper/niu_icml13.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. Yamada, and M. Sugiyama.<br>
                Information-theoretic semi-supervised metric learning via entropy regularization.<br>
                In <i>Proceedings of <a href="http://icml.cc/2012/" target="_blank">29th International Conference on Machine Learning (ICML'12)</a></i>,
                pp. 89--96, Edinburgh, Scotland, Jun 26--Jul 1, 2012.<br>
                [ <a href="paper/niu_icml12.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                T. Zhao, H. Hachiya, G. Niu, and M. Sugiyama.<br>
                Analysis and improvement of policy gradient estimation.<br>
                In <i><a href="http://nips.cc/Conferences/2011/" target="_blank">Advances in Neural Information Processing Systems 24 (NeurIPS'11)</a></i>,
                pp. 262--270, Granada, Spain, Dec 12--17, 2011.<br>
                [ <a href="paper/zhao_nips11.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                M. Yamada, G. Niu, J. Takagi, and M. Sugiyama.<br>
                Computationally efficient sufficient dimension reduction via squared-loss mutual information.<br>
                In <i>Proceedings of 3rd Asian Conference on Machine Learning (ACML'11)</i>,
                PMLR, vol. 20, pp. 247--262, Taoyuan, Taiwan, Nov 13--15, 2011.<br>
                [ <a href="paper/yamada_acml11.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, L. Shang, and M. Sugiyama.<br>
                Maximum volume clustering.<br>
                In <i>Proceedings of <a href="http://www.aistats.org/aistats2011/" target="_blank">14th International Conference on Artificial Intelligence and Statistics (AISTATS'11)</a></i>,
                PMLR, vol. 15, pp. 561--569, Fort Lauderdale, Florida, USA, Apr 11--13, 2011.<br>
                [ <a href="paper/niu_aistats11.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Dai, B. Hu, and G. Niu.<br>
                Bayesian maximum margin clustering.<br>
                In <i>Proceedings of <a href="http://datamining.it.uts.edu.au/icdm10/" target="_blank">10th IEEE International Conference on Data Mining (ICDM'10)</a></i>,
                pp. 108--117, Sydney, Australia, Dec 14--17, 2010.<br>
                [ <a href="paper/dai_icdm10.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, Y. Ji, and L. Shang.<br>
                Rough margin based core vector machine.<br>
                In <i>Proceedings of 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD'10)</i>,
                LNCS, vol. 6118, pp. 134--141, Hyderabad, India, Jun 21--24, 2010.<br>
                [ <a href="paper/niu_pakdd10.pdf" target="_blank">paper</a> ]
            </p></li>
            <li><p>
                B. Dai and G. Niu.<br>
                Compact margin machine.<br>
                In <i>Proceedings of 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD'10)</i>,
                LNCS, vol. 6119, pp. 507--514, Hyderabad, India, Jun 21--24, 2010.<br>
                [ <a href="paper/dai_pakdd10.pdf" target="_blank">paper</a> ]
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="journal"></a>Journal Articles</h2>
        <ol>
            <li><p>
                H. Sasaki, T. Kanamori, A. Hyvärinen, G. Niu, and M. Sugiyama.<br>
                Mode-seeking clustering and density ridge estimation via direct estimation of density-derivative-ratios.<br>
                <i><a href="http://jmlr.org/" target="_blank">Journal of Machine Learning Research</a></i>, vol. 18, no. 180, pp. 1--45, 2018.<br>
                [ <a href="http://jmlr.org/papers/v18/17-380.html"  target="_blank">link</a> ]
            </p></li>
            <li><p>
                T. Sakai, G. Niu, and M. Sugiyama.<br>
                Semi-supervised AUC optimization based on positive-unlabeled learning.<br>
                <i><a href="http://www.springer.com/computer/ai/journal/10994" target="_blank">Machine Learning</a></i>, vol. 107, no. 4, pp. 767--794, 2018.<br>
                [ <a href="http://link.springer.com/article/10.1007/s10994-017-5678-9" target="_blank">link</a> ]
            </p></li>
            <li><p>
                H. Sasaki, V. Tangkaratt, G. Niu, and M. Sugiyama.<br>
                Sufficient dimension reduction via direct estimation of the gradients of logarithmic conditional densities.<br>
                <i><a href="http://www.mitpressjournals.org/loi/neco/" target="_blank">Neural Computation</a></i>, vol. 30, no. 2, pp. 477--504, 2018.<br>
                [ <a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco_a_01035" target="_blank">link</a> ]
            </p></li>
            <li><p>
                M. C. du Plessis*, G. Niu*, and M. Sugiyama.<br>
                Class-prior estimation for learning from positive and unlabeled data.<br>
                <i><a href="http://www.springer.com/computer/ai/journal/10994" target="_blank">Machine Learning</a></i>, vol. 106, no. 4, pp. 463--492, 2017.<br>
                [ <a href="http://link.springer.com/article/10.1007/s10994-016-5604-6" target="_blank">link</a> ]
            </p></li>
            <li><p>
                H. Sasaki, Y.-K. Noh, G. Niu, and M. Sugiyama.<br>
                Direct density-derivative estimation.<br>
                <i><a href="http://www.mitpressjournals.org/loi/neco/" target="_blank">Neural Computation</a></i>,  vol. 28, no. 6, pp. 1101--1140, 2016.<br>
                [ <a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00835" target="_blank">link</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. Yamada, and M. Sugiyama.<br>
                Information-theoretic semi-supervised metric learning via entropy regularization.<br>
                <i><a href="http://www.mitpressjournals.org/loi/neco/" target="_blank">Neural Computation</a></i>,  vol. 26, no. 8, pp. 1717--1762, 2014.<br>
                [ <a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00614" target="_blank">link</a> ]
            </p></li>
            <li><p>
                D. Calandriello, G. Niu, and M. Sugiyama.<br>
                Semi-supervised information-maximization clustering.<br>
                <i><a href="http://www.journals.elsevier.com/neural-networks/" target="_blank">Neural Networks</a></i>, vol. 57, pp. 103--111, 2014.<br>
                [ <a href="http://www.sciencedirect.com/science/article/pii/S0893608014001245" target="_blank">link</a> ]
            </p></li>
            <li><p>
                M. Sugiyama, G. Niu, M. Yamada, M. Kimura, and H. Hachiya.<br>
                Information-maximization clustering based on squared-loss mutual information.<br>
                <i><a href="http://www.mitpressjournals.org/loi/neco/" target="_blank">Neural Computation</a></i>, vol. 26, no. 1, pp. 84--131, 2014.<br>
                [ <a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00534" target="_blank">link</a> ]
            </p></li>
            <li><p>
                G. Niu, B. Dai, L. Shang, and M. Sugiyama.<br>
                Maximum volume clustering: A new discriminative clustering approach.<br>
                <i><a href="http://jmlr.org/" target="_blank">Journal of Machine Learning Research</a></i>, vol. 14 (Sep), pp. 2641--2687, 2013.<br>
                [ <a href="http://jmlr.org/papers/v14/niu13a.html" target="_blank">link</a> ]
            </p></li>
            <li><p>
                T. Zhao, H. Hachiya, G. Niu, and M. Sugiyama.<br>
                Analysis and improvement of policy gradient estimation.<br>
                <i><a href="http://www.journals.elsevier.com/neural-networks/" target="_blank">Neural Networks</a></i>, vol. 26, pp. 118--129, 2012.<br>
                [ <a href="http://www.sciencedirect.com/science/article/pii/S0893608011002577" target="_blank">link</a> ]
            </p></li>
            <li><p>
                Y. Ji, J. Chen, G. Niu, L. Shang, and X. Dai.<br>
                Transfer learning via multi-view principal component analysis.<br>
                <i><a href="http://www.springer.com/computer/journal/11390" target="_blank">Journal of Computer Science and Technology</a></i>, vol. 26, no. 1, pp. 81--98, 2011.<br>
                [ <a href="http://link.springer.com/article/10.1007/s11390-011-9417-6" target="_blank">link</a> ]
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="workshop"></a>Workshop Presentations (selected)</h2>
        <ol>
            <li><p>
                G. Niu.<br>
                Robust learning against label noise.<br>
                Presented at <i><a href="http://indico2.riken.jp/event/3146/" target="_blank">The All-RIKEN Workshop 2019</a></i>, Wako, Japan, Dec 5--6, 2019.<br>
                <font color="#008800">(This was an award speech)</font><br>
            </p></li>
            <li><p>
                G. Niu.<br>
                When weakly-supervised learning meets deep learning.<br>
                Presented at <i><a href="http://www.ijcai-boom.org/" target="_blank">3rd IJCAI BOOM</a></i>, Stockholm, Sweden, Jul 13, 2018.<br>
                <font color="#008800">(This was an invited talk)</font><br>
            </p></li>
            <li><p>
                G. Niu.<br>
                When deep learning meets weakly-supervised learning.<br>
                Presented at <i><a href="http://www.ms.k.u-tokyo.ac.jp/TDLW2018/" target="_blank">Deep Learning: Theory, Algorithms, and Applications</a></i>, Tokyo, Japan, Mar 19--22, 2018.<br>
                <font color="#008800">(This was an invited talk)</font><br>
                [ <a href="paper/niu_tdlw2018.pdf" target="_blank">slides</a>,
                <a href="http://www.youtube.com/watch?v=onGqIZ1SHs0" target="_blank">video</a> ]
            </p></li>
            <li><p>
                G. Niu.<br>
                Statistical learning from weak supervision.<br>
                Presented at <i><a href="http://ircn.jp/en/archives/6603" target="_blank">1st IRCN Retreat 2018</a></i>, Yokohama, Japan, Mar 17--18, 2018.<br>
                <font color="#008800">(This was an invited talk)</font>
            </p></li>
            <li><p>
                G. Niu.<br>
                Recent advances on positive-unlabeled (PU) learning.<br>
                Presented at <i><a href="http://ibisml.org/ibisml030" target="_blank">30th IBISML (joint with PRMU and CVIM)</a></i>, Tokyo, Japan, Sep 15--16, 2017.<br>
                <font color="#008800">(This was an invited talk)</font><br>
                [ <a href="paper/niu_ibisml030.pdf" target="_blank">slides</a> ]
            </p></li>
            <li><p>
                G. Niu (presented by Tomoya Sakai).<br>
                Positive-unlabeled learning with application to semi-supervised learning.<br>
                Presented at <i><a href="http://www.microsoft.com/en-us/research/event/microsoft-research-asia-academic-day-2017/" target="_blank">Microsoft Research Asia Academic Day 2017</a></i>, Yilan, Taiwan, May 26, 2017.
            </p></li>
            <li><p>
                G. Niu, B. Dai, M. Yamada, and M. Sugiyama.<br>
                Information-theoretic semi-supervised metric learning via entropy regularization.<br>
                Presented at <i><a href="http://www.iip.ist.i.kyoto-u.ac.jp/mlss12/doku.php" target="_blank">21st MLSS</a></i>, Kyoto, Japan, Aug 27--Sep 7, 2012.
            </p></li>
            <li><p>
                G. Niu, B. Dai, L. Shang, and M. Sugiyama.<br>
                Maximum volume clustering.<br>
                Presented at <i><a href="http://mlss11.bordeaux.inria.fr/" target="_blank">18th MLSS</a></i>, Bordeaux, France, Sep 4--17, 2011.
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="thesis"></a>Theses</h2>
        <ol>
            <li><p>
                Gang Niu.<br>
                <i>Discriminative methods with imperfect supervision in machine learning</i> (204 pages).<br>
                Doctoral Thesis, Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan, Sep 2013.
            </p></li>
            <li><p>
                Gang Niu.<br>
                <i>Support vector learning based on rough set modeling</i> (71 pages in Chinese).<br>
                Master Thesis, Department of Computer Science and Technology, Nanjing University, Nanjing, China, May 2010.
            </p></li>
        </ol>
    </div>

</td>
</tr>
</table>
</body>
</html>
